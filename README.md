# Learning visual attributes on GQA subset
 
The project applys to serveral machine learning models on a subset of the GQA dataset for learning and relation[^1], and compares their performance and my understanding is in the report,but there is misunderstanding about PCA.

Also, for instruction please check the appendix in the report.

## Dataset
The dataset is based on a subset of the GQA dataset for learning attributes and relations.1 The GQA dataset consists of images where objects are annotated in terms of bounding boxes and relevant attributes and relations. Each row in our dataset corresponds to an object in one of the images and contains the following fields:
• the image ID
• the object ID
• position (x, y) and size (width, height) of the object’s bounding box
• colour of the object (categorical)
• texture of the object (categorical)
• colour histogram extracted from the bounding box. This comprises 9 values for each of the three components in the CIELAB colour space (intensity, red-green, blue-yellow), for a total of 27 values
• a histogram of oriented gradients (HOG) extracted from the bounding box,[^2] for a total of 288 values
• a set of complex cell responses based on oriented Gabor filters generated by the BIMP model,3 for a total of 126 values

The histograms and cell responses are calculated in 9 different areas of the bounding box to encode spatial characteristics of objects. Please see the file “features.png” for a rough illustration of the features.

To prevent data leakage, the dataset is splited in advance.


## Reference

[^1]:1D. A. Hudson and C. D. Manning, “GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,” CVPR 2019, pp. 6693-6702
[^2]N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection”. CVPR 2005